{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6151a767-919e-4900-ba4f-b6c2908951df",
   "metadata": {},
   "source": [
    "# Photo-Z Challenge: Machine Learning Pipeline\n",
    "Welcome to the interactive Photo-Z Challenge notebook! \n",
    "\n",
    "This notebook integrates the entire machine learning pipeline into a single, step-by-step educational flow. We will cover:\n",
    "1. Environment and Plotting Setup\n",
    "2. Data Preprocessing and Custom Datasets\n",
    "3. Model Architecture Details\n",
    "4. Training Loop Execution\n",
    "5. Validation and Metric Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f56531f5-d9e3-4bfb-8d70-2f30dfbc721b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6r/hl4s187n5b37619mh7d94nb40000gn/T/ipykernel_57744/3758340374.py:6: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from scipy.stats import gaussian_kde\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69e3f16d-3a8f-4b18-91c7-b925ff2abdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. PLOTTING STYLE SETTINGS ---\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'font.size': 16,          \n",
    "    'axes.labelsize': 20,     \n",
    "    'axes.titlesize': 22,     \n",
    "    'xtick.labelsize': 16,    \n",
    "    'ytick.labelsize': 16,    \n",
    "    'legend.fontsize': 16,    \n",
    "    'lines.linewidth': 4      \n",
    "})\n",
    "\n",
    "# Define custom colors\n",
    "TYPE_COLORS = {\n",
    "    'GALAXY_ID': 'tab:cyan',\n",
    "    'QSO': 'tab:red',\n",
    "    'GALAXY_OOD1': 'yellow',\n",
    "    'GALAXY_OOD2': 'gold',\n",
    "    'Passive': 'tab:orange',\n",
    "    'ELG': 'tab:green'\n",
    "}\n",
    "\n",
    "def get_smart_limits(data, padding=0.05):\n",
    "    \"\"\"Calculates axis limits based on data percentiles to filter outliers.\"\"\"\n",
    "    valid_data = data[np.isfinite(data)]\n",
    "    if len(valid_data) == 0:\n",
    "        return 0, 1\n",
    "    low = np.percentile(valid_data, 1)\n",
    "    high = np.percentile(valid_data, 99)\n",
    "    span = high - low\n",
    "    return low - span * padding, high + span * padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bd37e2-cb24-4f49-be52-0692932d4c5d",
   "metadata": {},
   "source": [
    "## Configuration & Data Preprocessing\n",
    "We define our helper functions to read the configuration file, handle missing data, and normalize our inputs. We also define the PyTorch `Dataset` that will feed our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfafbf5a-9863-4b06-9cf6-1c92df36c8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(config_path=\"./config.yaml\"):\n",
    "    with open(config_path, 'r') as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "def get_mad(series):\n",
    "    \"\"\"Calculates Median Absolute Deviation\"\"\"\n",
    "    median = series.median()\n",
    "    return (series - median).abs().median()\n",
    "\n",
    "def preprocess_data(df, config, train_stats=None):\n",
    "    \"\"\"Selects variables, fills NaNs, and normalizes using MAD.\"\"\"\n",
    "    selected_cols = []\n",
    "    for group in config['data']['selected_features']:\n",
    "        selected_cols.extend(config['data']['inputs'][group])\n",
    "    \n",
    "    X = df[selected_cols].copy()\n",
    "    \n",
    "    if 'Z' in df.columns:\n",
    "        y = df['Z'].values\n",
    "    else:\n",
    "        y = np.where(df['SPECTYPE'] == 2.0, df['Z_QSO'], df['Z_GAL'])\n",
    "\n",
    "    types = df['TYPE'].values\n",
    "    X = X.fillna(0.0)\n",
    "\n",
    "    cols_to_norm = []\n",
    "    for group in config['data']['features_to_normalize']:\n",
    "        if group in config['data']['selected_features']:\n",
    "             cols_to_norm.extend(config['data']['inputs'][group])\n",
    "    \n",
    "    if train_stats is None:\n",
    "        medians = X[cols_to_norm].median()\n",
    "        mads = X[cols_to_norm].apply(get_mad).replace(0, 1.0)\n",
    "        train_stats = {'medians': medians, 'mads': mads}\n",
    "    \n",
    "    X[cols_to_norm] = (X[cols_to_norm] - train_stats['medians']) / train_stats['mads']\n",
    "    \n",
    "    return X.values, y, types, train_stats, len(selected_cols)\n",
    "\n",
    "class PhotoZDataset(Dataset):\n",
    "    def __init__(self, features, targets, types):\n",
    "        self.features = torch.FloatTensor(features)\n",
    "        self.targets = torch.FloatTensor(targets).reshape(-1, 1)\n",
    "        self.types = np.array(types)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.targets[idx], self.types[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b451778a-fce8-491e-a853-588a21248380",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "The core model is a highly flexible Multilayer Perceptron (MLP) defined by `PhotoZNet`. We optimize it using a custom Delta Z Loss function tailored for photometric redshift challenges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82f8c72a-fe9d-4a69-ac12-ee2fdfefceeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhotoZNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, dropout_rates):\n",
    "        super(PhotoZNet, self).__init__()\n",
    "        layers = []\n",
    "        in_dim = input_size\n",
    "        \n",
    "        for h_dim, drop_rate in zip(hidden_layers, dropout_rates):\n",
    "            layers.append(nn.Linear(in_dim, h_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(drop_rate))\n",
    "            in_dim = h_dim\n",
    "            \n",
    "        layers.append(nn.Linear(in_dim, 1))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class DeltaZLoss(nn.Module):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        numerator = torch.abs(y_pred - y_true)\n",
    "        denominator = 1.0 + y_true\n",
    "        return torch.mean(numerator / denominator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f32dc9-6b83-407c-8dd8-a927b916acdc",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "This cell loads the HDF5 datasets, applies preprocessing, initializes the neural network, and runs the training loop over the specified epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b64464-ebb0-4f65-b10b-643eadf912dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10 [Train]:   0%|          | 0/4028 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cfg = load_config()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "df_val = pd.read_hdf(cfg['data']['val_path'], key='data')\n",
    "df_train = pd.read_hdf(cfg['data']['train_path'], key='data') \n",
    "df_val = pd.read_hdf(cfg['data']['val_path'], key='data')\n",
    "\n",
    "X_train, y_train, types_train, stats, input_dim = preprocess_data(df_train, cfg, train_stats=None)\n",
    "X_val, y_val, types_val, _, _ = preprocess_data(df_val, cfg, train_stats=stats)\n",
    "\n",
    "train_dataset = PhotoZDataset(X_train, y_train, types_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=cfg['data']['batch_size'], shuffle=True, drop_last=True)\n",
    "\n",
    "model = PhotoZNet(\n",
    "    input_size=input_dim,\n",
    "    hidden_layers=cfg['model']['hidden_layers'],\n",
    "    dropout_rates=cfg['model']['dropout_rates']\n",
    ").to(device)\n",
    "\n",
    "criterion = DeltaZLoss() if cfg['training']['loss_type'] == 'deltaz' else nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=cfg['training']['learning_rate'])\n",
    "\n",
    "epochs = cfg['training']['epochs']\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss_acc = 0.0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\", leave=False)\n",
    "    for inputs, targets, _ in pbar:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss_acc += loss.item()\n",
    "        pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_inputs = torch.FloatTensor(X_val).to(device)\n",
    "        val_targets = torch.FloatTensor(y_val).to(device)\n",
    "        val_outputs = model(val_inputs)\n",
    "        total_val_loss = criterion(val_outputs, val_targets).item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss: {train_loss_acc / len(train_loader):.4f} | Val Loss Global: {total_val_loss:.4f}\")\n",
    "\n",
    "os.makedirs(cfg['experiment']['save_dir'], exist_ok=True)\n",
    "save_path = os.path.join(cfg['experiment']['save_dir'], f\"{cfg['experiment']['group_name']}.pth\")\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Model saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca64ea6-ede8-4b06-8cc4-6e5cbf3e13f7",
   "metadata": {},
   "source": [
    "## Evaluation and Visualization\n",
    "Now we run inference on the validation set and calculate our primary metrics: Bias, $\\sigma_{NMAD}$, and Outlier Fraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a70c80-bb28-493f-a65e-3975e6659ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_binned(df, x_col, z_true_col='Z_TRUE', z_pred_col='Z_PRED', bins=None):\n",
    "    if bins is None:\n",
    "        return None, None, None, None\n",
    "    centers, bias_list, sigma_list, outlier_list = [], [], [], []\n",
    "    dz = (df[z_pred_col] - df[z_true_col]) / (1 + df[z_true_col])\n",
    "    \n",
    "    for i in range(len(bins) - 1):\n",
    "        mask = (df[x_col] >= bins[i]) & (df[x_col] < bins[i+1])\n",
    "        subset_dz = dz[mask]\n",
    "        \n",
    "        if len(subset_dz) < 100:\n",
    "            centers.append(np.nan); bias_list.append(np.nan); sigma_list.append(np.nan); outlier_list.append(np.nan)\n",
    "            continue\n",
    "            \n",
    "        centers.append(0.5 * (bins[i] + bins[i+1]))\n",
    "        bias_list.append(np.median(subset_dz))\n",
    "        sigma_list.append(1.4826 * np.median(np.abs(subset_dz - np.median(subset_dz))))\n",
    "        outlier_list.append(np.sum(np.abs(subset_dz) > 0.15) / len(subset_dz))\n",
    "\n",
    "    return np.array(centers), np.array(bias_list), np.array(sigma_list), np.array(outlier_list)\n",
    "\n",
    "def draw_metric_page(df, title_prefix, filter_condition=None):\n",
    "    data = df[filter_condition].copy() if filter_condition is not None else df.copy()\n",
    "    unique_types = data['TYPE'].unique()\n",
    "    \n",
    "    fig, axs = plt.subplots(2, 3, figsize=(22, 16))\n",
    "    fig.suptitle(title_prefix, fontsize=26, weight='bold')\n",
    "    \n",
    "    bins_mag = np.arange(17, 23.3 + 0.25, 0.25)\n",
    "    bins_z = np.arange(0, 1.8 + 0.1, 0.1)\n",
    "    \n",
    "    for t in unique_types:\n",
    "        subset = data[data['TYPE'] == t]\n",
    "        if len(subset) < 10: continue\n",
    "        lbl = f\"{t} (N={len(subset)})\"\n",
    "        col = TYPE_COLORS.get(t, 'white')\n",
    "        \n",
    "        # --- Row 1: vs MAG_i ---\n",
    "        x_m, bias_m, sig_m, out_m = compute_metrics_binned(subset, 'MAG_i', 'Z', 'Z_PRED', bins_mag)\n",
    "        axs[0, 0].plot(x_m, bias_m, label=lbl, color=col, marker='o', markersize=8)\n",
    "        axs[0, 1].plot(x_m, sig_m, label=lbl, color=col, marker='o', markersize=8)\n",
    "        out_m_log = (out_m * 100) + 1e-6\n",
    "        axs[0, 2].plot(x_m, out_m_log, label=lbl, color=col, marker='o', markersize=8)\n",
    "        \n",
    "        # --- Row 2: vs Z True ---\n",
    "        x_z, bias_z, sig_z, out_z = compute_metrics_binned(subset, 'Z', 'Z', 'Z_PRED', bins_z)\n",
    "        axs[1, 0].plot(x_z, bias_z, label=lbl, color=col, marker='o', markersize=8)\n",
    "        axs[1, 1].plot(x_z, sig_z, label=lbl, color=col, marker='o', markersize=8)\n",
    "        out_z_log = (out_z * 100) + 1e-6\n",
    "        axs[1, 2].plot(x_z, out_z_log, label=lbl, color=col, marker='o', markersize=8)\n",
    "\n",
    "    # --- Formatting ---\n",
    "    for ax in axs.flatten():\n",
    "        ax.grid(False)\n",
    "\n",
    "    for ax in axs[0, :]: \n",
    "        ax.set_xlabel('MAG_i')\n",
    "        ax.set_xlim(17, 23.3)\n",
    "    \n",
    "    for ax in axs[1, :]: \n",
    "        ax.set_xlabel('Z True')\n",
    "        ax.set_xlim(0, 1.8)\n",
    "        \n",
    "    axs[0, 0].set_ylabel('Bias $\\Delta z$'); axs[1, 0].set_ylabel('Bias $\\Delta z$')\n",
    "    \n",
    "    axs[0, 1].set_ylabel('$\\sigma_{NMAD}$ (log)'); axs[1, 1].set_ylabel('$\\sigma_{NMAD}$ (log)')\n",
    "    axs[0, 1].set_yscale('log'); axs[1, 1].set_yscale('log')\n",
    "    \n",
    "    axs[0, 2].set_ylabel('Outlier % (log)'); axs[1, 2].set_ylabel('Outlier % (log)')\n",
    "    axs[0, 2].set_yscale('log'); axs[1, 2].set_yscale('log')\n",
    "\n",
    "    axs[0, 0].legend(loc='lower left', frameon=True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run Evaluation and Plotting\n",
    "df_val['Z_PRED'] = model(torch.FloatTensor(X_val).to(device)).cpu().detach().numpy().flatten()\n",
    "if 'Z' not in df_val.columns:\n",
    "    df_val['Z'] = np.where(df_val['SPECTYPE'] == 2.0, df_val['Z_QSO'], df_val['Z_GAL'])\n",
    "\n",
    "draw_metric_page(df_val, \"Performance Metrics (All Objects)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee985c5a-207a-440a-a159-f685d2eb4ac8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
